{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   abbreviated        ai  analysis       and       are  artificial        as   \n",
      "0     0.000000  0.000000  0.000000  0.276864  0.000000    0.000000  0.000000  \\\n",
      "1     0.000000  0.000000  0.000000  0.232042  0.000000    0.000000  0.000000   \n",
      "2     0.354187  0.299152  0.000000  0.000000  0.000000    0.260104  0.354187   \n",
      "3     0.000000  0.000000  0.000000  0.201008  0.000000    0.000000  0.000000   \n",
      "4     0.000000  0.224569  0.000000  0.000000  0.000000    0.000000  0.000000   \n",
      "5     0.000000  0.000000  0.267326  0.173456  0.000000    0.000000  0.000000   \n",
      "6     0.000000  0.000000  0.000000  0.000000  0.000000    0.373015  0.000000   \n",
      "7     0.000000  0.000000  0.000000  0.000000  0.406738    0.000000  0.000000   \n",
      "8     0.000000  0.000000  0.000000  0.000000  0.000000    0.426023  0.000000   \n",
      "\n",
      "        big     broad     build  ...      test      text      that       the   \n",
      "0  0.000000  0.000000  0.000000  ...  0.213348  0.213348  0.000000  0.360394  \\\n",
      "1  0.000000  0.000000  0.000000  ...  0.000000  0.000000  0.000000  0.000000   \n",
      "2  0.000000  0.000000  0.000000  ...  0.000000  0.000000  0.000000  0.000000   \n",
      "3  0.000000  0.000000  0.000000  ...  0.000000  0.000000  0.000000  0.261653   \n",
      "4  0.000000  0.000000  0.000000  ...  0.000000  0.000000  0.224569  0.000000   \n",
      "5  0.267326  0.000000  0.000000  ...  0.000000  0.000000  0.225788  0.000000   \n",
      "6  0.000000  0.000000  0.000000  ...  0.000000  0.000000  0.000000  0.000000   \n",
      "7  0.000000  0.000000  0.406738  ...  0.000000  0.000000  0.000000  0.000000   \n",
      "8  0.000000  0.580121  0.000000  ...  0.000000  0.000000  0.000000  0.000000   \n",
      "\n",
      "       this        to  understanding      used      with      work  \n",
      "0  0.213348  0.156676       0.000000  0.000000  0.000000  0.213348  \n",
      "1  0.000000  0.000000       0.000000  0.000000  0.000000  0.000000  \n",
      "2  0.000000  0.000000       0.000000  0.000000  0.000000  0.000000  \n",
      "3  0.000000  0.000000       0.000000  0.000000  0.000000  0.000000  \n",
      "4  0.000000  0.000000       0.265883  0.000000  0.265883  0.000000  \n",
      "5  0.000000  0.196316       0.000000  0.225788  0.000000  0.000000  \n",
      "6  0.000000  0.000000       0.000000  0.000000  0.000000  0.000000  \n",
      "7  0.000000  0.298696       0.000000  0.343537  0.000000  0.000000  \n",
      "8  0.000000  0.000000       0.000000  0.000000  0.000000  0.000000  \n",
      "\n",
      "[9 rows x 72 columns]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import nltk\n",
    "from nltk import tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "vec=TfidfVectorizer()\n",
    "text_db=['ok this is just a text to test the feature extarction and prioritization process. So lets check does it work and see the output',\n",
    "         \"Machine learning is a fascinating field. It combines computer science and statistics.\",\n",
    "         \"Artificial intelligence, often abbreviated as AI, is a rapidly evolving technology.\",\n",
    "         \"Data science is the practice of extracting knowledge and insights from data.\",\n",
    "         \"Natural language processing (NLP) is a subfield of AI that deals with human language understanding.\",\n",
    "         \"Big data is a term used to describe large and complex datasets that require specialized techniques for analysis.\",\n",
    "         \"Machine learning is a subset of artificial intelligence.\",\n",
    "         \"Machine learning techniques are used to build models.\",\n",
    "         \"Artificial intelligence is a broad field.\"]\n",
    "tf_idf=vec.fit_transform(text_db)\n",
    "print(pd.DataFrame(tf_idf.toarray(), columns=vec.get_feature_names_out()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['ok', 'this', 'is', 'just', 'a', 'text', 'to', 'test', 'the', 'feature', 'extarction', 'and', 'prioritization', 'process', '.', 'So', 'lets', 'check', 'does', 'it', 'work', 'and', 'see', 'the', 'output']\n",
      "['Machine', 'learning', 'is', 'a', 'fascinating', 'field', '.', 'It', 'combines', 'computer', 'science', 'and', 'statistics', '.']\n",
      "['Artificial', 'intelligence', ',', 'often', 'abbreviated', 'as', 'AI', ',', 'is', 'a', 'rapidly', 'evolving', 'technology', '.']\n",
      "['Data', 'science', 'is', 'the', 'practice', 'of', 'extracting', 'knowledge', 'and', 'insights', 'from', 'data', '.']\n",
      "['Natural', 'language', 'processing', '(', 'NLP', ')', 'is', 'a', 'subfield', 'of', 'AI', 'that', 'deals', 'with', 'human', 'language', 'understanding', '.']\n",
      "['Big', 'data', 'is', 'a', 'term', 'used', 'to', 'describe', 'large', 'and', 'complex', 'datasets', 'that', 'require', 'specialized', 'techniques', 'for', 'analysis', '.']\n",
      "['Machine', 'learning', 'is', 'a', 'subset', 'of', 'artificial', 'intelligence', '.']\n",
      "['Machine', 'learning', 'techniques', 'are', 'used', 'to', 'build', 'models', '.']\n",
      "['Artificial', 'intelligence', 'is', 'a', 'broad', 'field', '.']\n"
     ]
    }
   ],
   "source": [
    "tokens=[]\n",
    "for i in text_db:\n",
    "    token_append=nltk.word_tokenize(i)\n",
    "    tokens.append(token_append)\n",
    "for document in tokens:\n",
    "    print (document)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['ok this is just a text to test the feature extarction and prioritization process.', 'So lets check does it work and see the output']\n",
      "['Machine learning is a fascinating field.', 'It combines computer science and statistics.']\n",
      "['Artificial intelligence, often abbreviated as AI, is a rapidly evolving technology.']\n",
      "['Data science is the practice of extracting knowledge and insights from data.']\n",
      "['Natural language processing (NLP) is a subfield of AI that deals with human language understanding.']\n",
      "['Big data is a term used to describe large and complex datasets that require specialized techniques for analysis.']\n",
      "['Machine learning is a subset of artificial intelligence.']\n",
      "['Machine learning techniques are used to build models.']\n",
      "['Artificial intelligence is a broad field.']\n"
     ]
    }
   ],
   "source": [
    "token_words=[]\n",
    "for i in text_db:\n",
    "    token_append=nltk.sent_tokenize(i)\n",
    "    token_words.append(token_append)\n",
    "for document in token_words:\n",
    "    print (document)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['ok', 'this', 'is', 'just', 'a', 'text', 'to', 'test', 'the', 'feature', 'extarction', 'and', 'prioritization', 'process', '.', 'So', 'lets', 'check', 'does', 'it', 'work', 'and', 'see', 'the', 'output']\n",
      "['Machine', 'learning', 'is', 'a', 'fascinating', 'field', '.', 'It', 'combines', 'computer', 'science', 'and', 'statistics', '.']\n",
      "['Artificial', 'intelligence', ',', 'often', 'abbreviated', 'as', 'AI', ',', 'is', 'a', 'rapidly', 'evolving', 'technology', '.']\n",
      "['Data', 'science', 'is', 'the', 'practice', 'of', 'extracting', 'knowledge', 'and', 'insights', 'from', 'data', '.']\n",
      "['Natural', 'language', 'processing', '(', 'NLP', ')', 'is', 'a', 'subfield', 'of', 'AI', 'that', 'deals', 'with', 'human', 'language', 'understanding', '.']\n",
      "['Big', 'data', 'is', 'a', 'term', 'used', 'to', 'describe', 'large', 'and', 'complex', 'datasets', 'that', 'require', 'specialized', 'techniques', 'for', 'analysis', '.']\n",
      "['Machine', 'learning', 'is', 'a', 'subset', 'of', 'artificial', 'intelligence', '.']\n",
      "['Machine', 'learning', 'techniques', 'are', 'used', 'to', 'build', 'models', '.']\n",
      "['Artificial', 'intelligence', 'is', 'a', 'broad', 'field', '.']\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "token_words_filtered=[]\n",
    "for i in text_db:\n",
    "    token_append = word_tokenize(i)\n",
    "    token_words_filtered.append(token_append)\n",
    "for document in token_words_filtered:\n",
    "    print(document)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'list' object has no attribute 'casefold'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\Niitro_musics\\Desktop\\Coding\\AI dev_stud\\my_code-stud-exp\\test-codes\\Word-Text tokenization 2.ipynb Cell 5\u001b[0m line \u001b[0;36m5\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Niitro_musics/Desktop/Coding/AI%20dev_stud/my_code-stud-exp/test-codes/Word-Text%20tokenization%202.ipynb#W5sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m token_words:\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Niitro_musics/Desktop/Coding/AI%20dev_stud/my_code-stud-exp/test-codes/Word-Text%20tokenization%202.ipynb#W5sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m      \u001b[39mfor\u001b[39;00m word \u001b[39min\u001b[39;00m token_words_filtered:\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/Niitro_musics/Desktop/Coding/AI%20dev_stud/my_code-stud-exp/test-codes/Word-Text%20tokenization%202.ipynb#W5sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m           \u001b[39mif\u001b[39;00m word\u001b[39m.\u001b[39;49mcasefold() \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m stop_words:\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Niitro_musics/Desktop/Coding/AI%20dev_stud/my_code-stud-exp/test-codes/Word-Text%20tokenization%202.ipynb#W5sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m                filtered_list\u001b[39m.\u001b[39mappend(word)\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Niitro_musics/Desktop/Coding/AI%20dev_stud/my_code-stud-exp/test-codes/Word-Text%20tokenization%202.ipynb#W5sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m filtered_list \u001b[39m=\u001b[39m [\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Niitro_musics/Desktop/Coding/AI%20dev_stud/my_code-stud-exp/test-codes/Word-Text%20tokenization%202.ipynb#W5sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m      word \u001b[39mfor\u001b[39;00m word \u001b[39min\u001b[39;00m token_words_filtered \u001b[39mif\u001b[39;00m word\u001b[39m.\u001b[39mcasefold() \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m stop_words\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Niitro_musics/Desktop/Coding/AI%20dev_stud/my_code-stud-exp/test-codes/Word-Text%20tokenization%202.ipynb#W5sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m ]\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'list' object has no attribute 'casefold'"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "token_words_filtered=[nltk.word_tokenize(append) for append in text_db]\n",
    "stop_words = set(stopwords.words(\"english\"))\n",
    "filtered_list=[]\n",
    "\n",
    "for i, sentence in enumerate(text_db):\n",
    "     \n",
    "     for word.casefold() not in stop_words:\n",
    "          filtered_list.append(word)\n",
    "          \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['ok', 'text', 'test', 'feature', 'extarction', 'prioritization', 'process', '.', 'lets', 'check', 'work', 'see', 'output', 'Machine', 'learning', 'fascinating', 'field', '.', 'combines', 'computer', 'science', 'statistics', '.', 'Artificial', 'intelligence', ',', 'often', 'abbreviated', 'AI', ',', 'rapidly', 'evolving', 'technology', '.', 'Data', 'science', 'practice', 'extracting', 'knowledge', 'insights', 'data', '.', 'Natural', 'language', 'processing', '(', 'NLP', ')', 'subfield', 'AI', 'deals', 'human', 'language', 'understanding', '.', 'Big', 'data', 'term', 'used', 'describe', 'large', 'complex', 'datasets', 'require', 'specialized', 'techniques', 'analysis', '.', 'Machine', 'learning', 'subset', 'artificial', 'intelligence', '.', 'Machine', 'learning', 'techniques', 'used', 'build', 'models', '.', 'Artificial', 'intelligence', 'broad', 'field', '.']\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "token_words_filtered = [nltk.word_tokenize(sentence) for sentence in text_db]\n",
    "stop_words = nltk.corpus.stopwords.words(\"english\")\n",
    "\n",
    "filtered_list = []\n",
    "for i, sentence in enumerate(text_db):\n",
    "    for word in token_words_filtered[i]:\n",
    "        if word.casefold() not in stop_words:\n",
    "            filtered_list.append(word)\n",
    "\n",
    "print(filtered_list)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
